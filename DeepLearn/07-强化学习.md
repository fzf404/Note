<!-- 
title: 07-强化学习
sort: 
--> 

```python
import numpy as np
import pandas as pd
import time

sizes = 6				# 终点的距离
# 行动方式
actions = ['left','right']
less = 0.9			# 衰减奖励
lr = 0.1				# 学习速率
discount = 0.9	# 折扣因子
rounds = 13			# 最大回合
step_time = 0.3	# 每步时间

# 生成q表
def build_q_table(sizes, actions):
  # 2*6的表格，列名由action命名
  table = pd.DataFrame(
      np.zeros((sizes, len(actions))),
      columns=actions,
  )
  return table

# 选择动作(位置，q表)
def choose_action(state, q_table):
  # 取出对应行的全部数据
  state_actions = q_table.iloc[state,:]
  # 假如随机值大于less 或 action全为0 随机选择action
  if (np.random.uniform() > less) or ((state_actions == 0).all()):
    action_name = np.random.choice(actions)
  else:
    # 取出最大值对应的id(列名
    action_name = state_actions.idxmax()    
  return action_name

# 环境对行为的评分
def get_env_feedback(state, action):
  # 向右触发的奖励
  if action == 'right':
    # 距离足够近时触发奖励
    if state == sizes - 2:
      next_state = 'terminal'
      reward = 1
    else:
      next_state = state + 1
      reward = 0
  else:
    reward = 0
    if state == 0:
      next_state = state  # reach the wall
    else:
      next_state = state - 1
  return next_state, reward

# 更新环境 [位置 回合数 步数]
def update_env(state, round, step_counts):
  env_list = ['-']*(sizes-1) + ['T']
  if state == 'terminal':
    interaction = 'round %s: total_steps = %s' % (round+1, step_counts)
    print('\r{}'.format(interaction), end='')
    time.sleep(2)
    print('\r                                ', end='')
  else:
    env_list[state] = 'o'
    interaction = ''.join(env_list)
    print('\r{}'.format(interaction), end='')
    time.sleep(step_time)

# 开始学习
def rl():
  q_table = build_q_table(sizes, actions)
  # 回合数
  for round in range(rounds):
    step_counts = 0  # 步数
    state = 0     # 状态
    is_terminated = False
    update_env(state, round, step_counts)        # 控制台展示输出
    while not is_terminated:
      action = choose_action(state, q_table)    # 选择行动
      next_state, reward = get_env_feedback(state, action) # 更新q表
      q_predict = q_table.loc[state, action]    # 学习前的值
      if next_state != 'terminal':
        # 求实际激励值
        q_target = reward + discount * q_table.iloc[next_state,:].max()
      else:
        q_target = reward
        is_terminated = True 
      # 更新q表
      q_table.loc[state,action] += lr * (q_target - q_predict)
      state = next_state

      update_env(state, round, step_counts)
      step_counts += 1
  return q_table

if __name__ == "__main__":
	q_table = rl()
  print(q_tabel)
```

