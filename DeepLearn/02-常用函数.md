 <!-- 
title: 02-常用函数
sort: 
-->

## 激活函数

![image-20210130161646295](https://gitee.com/nmdfzf404/Image-hosting/raw/master/2021/image-20210130161646295.png)

- Leaky ReLU

![img](https://gitee.com/nmdfzf404/Image-hosting/raw/master/2021/u=2147037458,4155023785&fm=26&gp=0.jpg)

- SELU

  ![img](https://gitee.com/nmdfzf404/Image-hosting/raw/master/2021/u=2448423465,3835849782&fm=11&gp=0.jpg)

- softplus

  ![img](https://gitee.com/nmdfzf404/Image-hosting/raw/master/2021/u=2661888347,1277836998&fm=11&gp=0.jpg)

## 损失函数

> 衡量输出值与输入值的损失
>
> $loss=(wx+b-y)^2$

| torch.nn         | torch.nn.functional (F) | 公式                          |
| :--------------- | :---------------------- | ----------------------------- |
| CrossEntropyLoss | cross_entropy           | $−\sum_{k=1}^N(p_k∗\log q_k)$ |
| LogSoftmax       | log_softmax             |                               |
| NLLLoss          | nll_loss                | $f(x,class)=−x[class]$        |

### Softmax

> `归一化指数函数`
>
> 将数据转变成相加为 1 的概率

$ S*{i} = \frac{e^i}{\sum*{j}e^j} $

### LogSoftmax

> **log**和**softmax**合并执行

### CrossEntropyLoss

> 交叉熵

## 检验优化算法

- Himmelblau

  $ f(x,y)=(x^2+y-11)^2+(x+y^2-7)^2 $
