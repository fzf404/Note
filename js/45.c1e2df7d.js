(window.webpackJsonp=window.webpackJsonp||[]).push([[45],{673:function(n,e){n.exports="\x3c!--\ntitle: 20-LeNet网络\nsort:\n--\x3e\n\n## LeNet-5\n\n> 典型的卷积神经网络的结构\n\n![从零开始实现一个基于Pytorch的卷积神经网络](https://img-1257284600.cos.ap-beijing.myqcloud.com/2021/OYoihxStFCslGzw.jpg)\n\n1. C1-卷积层：6 个 5\\*5 的卷积核，产生 6 个大小为 28x28 的特征图\n2. S2-池化层：2x2 最大池化进行下采样，产生 6 个大小为 14\\*14 的滤波图\n3. C3-卷积层：16 个 10\\*10 的特征图\n4. S4-池化层：16 个大小为 5\\*5 的滤波图\n5. C5-卷积层：卷积层，也可为全连接层，16\\*5\\*5=129\n6. F6-全连接层：输入为 120 维向量，输出为 84 维向量\n7. OUTPUT-输出层：输入为 84 维向量，输出为 10 维向量\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass LeNet(nn.Module):\n  def __init__(self):\n    # 初始化父类\n    super(LeNet, self).__init__()\n    # 定义模型中的每一层\n    self.C1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2)\n    self.R1 = nn.ReLU()\n    self.S2 = nn.MaxPool2d(kernel_size=2)\n    self.C3 = nn.Conv2d(6, 16, 5, 1, 0)\n    self.R2 = nn.ReLU()\n    self.S4 = nn.MaxPool2d(2)\n    self.C5 = nn.Conv2d(16, 120, 5, 1, 0)\n    self.R3 = nn.ReLU()\n    self.F6 = nn.Linear(in_features=120, out_features=84)\n    self.R4 = nn.ReLU()\n    self.OUT = nn.Linear(84, 10)\n  # 向前传播\n  def forward(self, x):\n    x = self.C1(x)\n    x = self.R1(x)\n    x = self.S2(x)\n    x = self.C3(x)\n    x = self.R2(x)\n    x = self.S4(x)\n    x = self.C5(x)\n    x = self.R3(x)\n    x = x.view(x.size(0), -1)\n    x = self.F6(x)\n    x = self.R4(x)\n    x = self.OUT(x)\n    return x\n\n# 导入数据集\nimport torchvision\ntorchvision.datasets.MNIST('./data', download=True)\n\n# 训练开始\nimport torchvision\nimport torch.utils.data as Data\n\nmodel = LeNet()\n\nEpoch = 5\nbatch_size = 64\nlr = 0.001\n\ntrain_data = torchvision.datasets.MNIST(root='./data/', train=True, transform=torchvision.transforms.ToTensor(), download=False)\n# 损失函数和优化器\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\ntrain_loader = Data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n\n# 启动梯度\ntorch.set_grad_enabled(True)\nmodel.train()\n\n# GPU加速\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor epoch in range(Epoch):\n  # 损失值及准确率\n  running_loss = 0.0\n  acc = 0.0\n  for step, data in enumerate(train_loader):\n    x, y = data\n    optimizer.zero_grad() # 梯度清零\n    y_pred = model(x.to(device, torch.float)) # 预测值\n    loss = loss_function(y_pred, y.to(device, torch.long))  # 损失值\n    loss.backward() # 向后传播\n\n    running_loss += float(loss.data.cpu())\n    pred = y_pred.argmax(dim=1)\n    acc += (pred.data.cpu() == y.data).sum()\n\n    # 打印当前状态\n    if step % 100 == 99:\n      loss_avg = running_loss / (step + 1)\n      acc_avg = float(acc / ((step + 1) * batch_size))\n      print('Epoch', epoch + 1, ',step', step + 1, '| Loss_avg: %.4f' % loss_avg, '|Acc_avg:%.4f' % acc_avg)\n\n    optimizer.step()  # 参数更新\n\n# 保存模型\ntorch.save(model, './LeNet.pkl')\n\n# 测试\ntest_data = torchvision.datasets.MNIST(root='./data/', train=False, transform=torchvision.transforms.ToTensor(), download=False)\ntest_loader = Data.DataLoader(test_data, batch_size=1, shuffle=False)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nnet = torch.load('./LeNet.pkl',map_location=torch.device(device))\nnet.to(device)\n\n# 关闭梯度更新\ntorch.set_grad_enabled(False)\nnet.eval()\n\nlength = test_data.data.size(0)\nacc = 0.0\n\nfor i, data in enumerate(test_loader):\n  x, y = data\n  y_pred = net(x.to(device, torch.float))\n  pred = y_pred.argmax(dim=1)\n  acc += (pred.data.cpu() == y.data).sum()\n  # print('Predict:', int(pred.data.cpu()), '|Ground Truth:', int(y.data))\n\nacc = (acc / length) * 100\nprint('Accuracy: %.2f' %acc, '%')\n```\n"}}]);