(window.webpackJsonp=window.webpackJsonp||[]).push([[47],{675:function(n,t){n.exports="\x3c!--\ntitle: 30-强化学习\nsort:\n--\x3e\n\n```python\nimport numpy as np\nimport pandas as pd\nimport time\n\nsizes = 6\t\t\t\t# 终点的距离\n# 行动方式\nactions = ['left','right']\nless = 0.9\t\t\t# 按照价值决定行为的概率\nlr = 0.1\t\t\t\t# 学习速率\ndiscount = 0.9\t# 折扣因子\nrounds = 13\t\t\t# 最大回合\nstep_time = 0.3\t# 每步时间\n\n# 生成q表\ndef build_q_table(sizes, actions):\n  # 2*6的表格，列名由action命名\n  table = pd.DataFrame(\n      np.zeros((sizes, len(actions))),\n      columns=actions,\n  )\n  return table\n\n# 选择动作(位置，q表)\ndef choose_action(state, q_table):\n  # 取出对应行的全部数据\n  state_actions = q_table.iloc[state,:]\n  # 假如随机值大于 less 或 action 全为 0 随机选择 action\n  if (np.random.uniform() > less) or ((state_actions == 0).all()):\n    action_name = np.random.choice(actions)\n  else:\n    # 取出最大值对应的id(列名\n    action_name = state_actions.idxmax()\n  return action_name\n\n# 环境对行为的评分\ndef get_env_feedback(state, action):\n  # 向右触发的奖励\n  if action == 'right':\n    # 距离足够近时触发奖励\n    if state == sizes - 2:\n      next_state = 'terminal'\n      reward = 1\n    else:\n      next_state = state + 1\n      reward = 0\n  else:\n    reward = 0\n    if state == 0:\n      next_state = state  # 是否到边缘\n    else:\n      next_state = state - 1\n  return next_state, reward\n\n# 更新环境 [位置 回合数 步数]\ndef update_env(state, round, step_counts):\n  env_list = ['-']*(sizes-1) + ['T']\n  if state == 'terminal':\n    interaction = 'round %s: total_steps = %s' % (round+1, step_counts)\n    print('\\r{}'.format(interaction), end='')\n    time.sleep(2)\n    print('\\r                                ', end='')\n  else:\n    env_list[state] = 'o'\n    interaction = ''.join(env_list)\n    print('\\r{}'.format(interaction), end='')\n    time.sleep(step_time)\n\n# 开始学习\ndef rl():\n  q_table = build_q_table(sizes, actions)\n  # 回合数\n  for round in range(rounds):\n    step_counts = 0  # 步数\n    state = 0     # 状态\n    is_terminated = False\n    update_env(state, round, step_counts)        # 控制台展示输出\n    while not is_terminated:\n      action = choose_action(state, q_table)    # 选择行动\n      next_state, reward = get_env_feedback(state, action) # 更新q表\n      q_predict = q_table.loc[state, action]    # 学习前的值\n      if next_state != 'terminal':\n        # 求实际激励值\n        q_target = reward + discount * q_table.iloc[next_state,:].max()\n      else:\n        q_target = reward\n        is_terminated = True\n      # 更新q表\n      q_table.loc[state,action] += lr * (q_target - q_predict)\n      state = next_state\n\n      update_env(state, round, step_counts)\n      step_counts += 1\n  return q_table\n\nif __name__ == \"__main__\":\n\tq_table = rl()\n  print(q_tabl)\n```\n"}}]);