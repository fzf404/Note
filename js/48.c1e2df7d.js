(window.webpackJsonp=window.webpackJsonp||[]).push([[48],{676:function(t,n){t.exports="\x3c!--\ntitle: 41-NLP自然语言处理\nsort:\n--\x3e\n\n### python 基础\n\n```python\nstr.strip()\t\t# 去除某个字符\nstr.replace()\t\t# 替换全部\nstr.find()\t\t# 查找\nstr.isxx()\t\t# 判断\nstr.splite()\t# 分割为列表\nlist.join(cc)\t# 列表合并为字符串\n\n# 正则\npat = re.compile(r'')\nre.findall(pat, str)\t# 筛选出全部\n# 替换与分割\nre.sub(pat, '', str)\t# 替换\nre.subn(pat, '', str)\t# 输出替换次数\nre.splite(pat, str)\t\t# 分割\n\n# 查找与分组\nre.search(pat, str)\t\t# 查找\n↑.group()\t\t# 索引位置\n\n# 使用(?P<name>[])方式创建别名\npat = re.compile(r'(?P<let>[a-z]+)(?P<num>[0-9]+)')\nm = re.search(pat,str)\t# 查找\nm.group('let')\t\t\t\t\t# 别名对应\nm.groups()\t\t\t\t\t\t\t# 全部组列表\n```\n\n### nltk\n\n```python\nimport nltk\nnltk.download('xx')\t\t# 安装扩展包\n\n# 单词分割\nfrom nltk import word_tokenize\n\ninput_str = \"I'm your father every day.\"\ntokens = word_tokenize(input_str)\n\n# 转化为Text对象\nfrom nltk.text import Text\n\nt = Text(tokens)\nt.count('your')\t\t# 单词个数\nt.index('every')\t# 单词位置\nt.plot(8)\t\t\t\t\t# 统计词频\n\n# 停用词\nfrom nltk.corpus import stopwords\n\nstopwords.fileids()\t\t# 支持的语言\n# 求交集\nset(tokens).intersection(set(stopwords.words('english')))\n# 过滤停用词\n[w for w in tokens if w not in stopwords.words('english')]\n\n# 词性\nfrom nltk import pos_tag\n\ntags = pos_tag(tokens)\n\n# 词性匹配\nimport svgling\t\t# 绘图库\nfrom nltk import RegexpParser\n\ngrammer = \"DEMO: {<VB>?<VBP>*}\"\t# 正则\ncp = RegexpParser(grammer)\t\t\t# 分析器\nresult = cp.parse(tags)\t\t\t\t\t# 解析\n\nsvgling.draw_tree(result)\t\t\t\t# 树状图\n```\n\n### SpaCy\n\n```python\nimport spacy\nnlp = spacy.load('en')\n\ndoc=nlp(input_str)\n\n[token for token in doc]\t\t\t# 分词\n[sent for sent in doc.sents]\t# 分句\n[(token,token.pos_) for token in doc]\t\t# 词性\n[(ent,ent.label_) for ent in doc.ents]\t# 命名体\n\n# 可视化展示\ndisplacy.render(doc,style='ent',jupyter=True)\n\n# 词频统计\nfrom collections import Counter\n\nc = Counter()\nfor ent in doc.ents:\n  if ent.label_ == 'PERSON':\n    c[ent.lemma_]+=1\n\nc.most_common(20)\n```\n\n### Jieba\n\n> [文档](https://github.com/fxsjy/jieba)\n\n```python\nstr = \"他来到了网易杭研大厦\"\njieba.enable_paddle()\t\t# 使用神经网络\n\n# 分词\nseg_list = jieba.cut(str, use_paddle=True)\n[i for i in seg_list]\n\n# 添加词典\njieba.load_userdict(\"userdict.txt\")\njieba.add_word(\"\")\n\n# 分析词频\nimport jieba.analyse\ntags = jieba.analyse.extract_tags(str,topK=5,withWeight=True)\n```\n\n### wordcloud\n\n```python\n# 词频统计\ncount = jieba.analyse.extract_tags(file_text,topK=200,withWeight=True)\n# 遮罩\nmask = np.array(Image.open('mask.png'))\n# 绘图\nmy_wordcloud = WordCloud(\n  background_color='white',\t\t\t\t\t\t# 背景颜色\n  max_words=100,\t\t\t\t\t\t\t\t\t\t\t# 最多词数\n  font_path='Source Han Serif.otf',\t\t# 字体文件\n  mask=mask,\t\t\t\t\t\t\t\t\t\t\t\t\t# 遮罩\n  width=1920,\t\t\t\t\t\t\t\t\t\t\t\t\t# 长宽\n  height=1080,\n).generate_from_frequencies(dict(count))\t# 词频字典\nmy_wordcloud.to_file('result.jpg')\t\t# b\n```\n"}}]);