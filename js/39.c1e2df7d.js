(window.webpackJsonp=window.webpackJsonp||[]).push([[39],{667:function(n,o){n.exports="\x3c!--\ntitle: 11-Pytorch基础\nsort:\n--\x3e\n\n## 计算\n\n```python\nimport torch.nn as nn\n\n\n# 算均方误差\nx = torch.ones(1)\nw = torch.full([1],3.)\nloss_func = nn.MSELoss()\n\n\n# 将w变为可导\nw.requires_grad_()\n# 更新计算图 (predict, label)\nmse = loss_func(torch.ones(1),x*w)\n\n# 求导\ntorch.autograd.grad(mse,[w])\n# 更简单的向后传播\nmse.backward()\nw.grad\n\n# 归一化\nx = torch.tensor([1,2,3],dtype=torch.float)\nsoftmax1 = nn.Softmax(dim=0)\nsoftmax1(x)\n```\n\n## 优化函数\n\n```python\ndef himmelblau(x):\n  return (x[0]**2+x[1]-11)**2+(x[0]+x[1]**2-7)**2\nx = np.arange(-6,6,0.1)\ny = np.arange(-6,6,0.1)\n# 转换为网格点坐标矩阵\nX, Y = np.meshgrid(x,y)\nZ = himmelblau([X,Y])\n\nxy=torch.tensor([0.,0.],requires_grad=True)\n# Adam优化算法\noptimizer = torch.optim.Adam([xy],lr=1e-2)\nfor step in range(20000):\n  # 求预测值\n  pred=himmelblau(xy)\n  # 梯度设置为0\n  optimizer.zero_grad()\n  # 向后传播\n  pred.backward()\n  optimizer.step()\n  if step%2000==0:\n    print ('step {}:xy={},f(x)={}'.format(step,xy.tolist(),pred.item()))\n\n```\n\n## 交叉熵\n\n> `cross entropy`: 分类问题优势-收敛快\n\n```python\na1 = torch.full([4],1/4.)\na2 = torch.tensor([0.1,0.1,0.1,0.7])\na3 = torch.tensor([0.001,0.001,0.001,0.997])\n# 求熵\n-(a1*torch.log2(a1)).sum()    # 2\n-(a2*torch.log2(a2)).sum()    # 1.36\n-(a3*torch.log2(a3)).sum()    # 0.03\n```\n\n## 项数影响\n\n![image-20210225202252994](https://img-1257284600.cos.ap-beijing.myqcloud.com/2021/image-20210225202252994.png)\n\n## 欠拟合与过拟合\n\n![image-20210225204202533](https://img-1257284600.cos.ap-beijing.myqcloud.com/2021/image-20210225204202533.png)\n\n### 正则化\n\n> Regularization / Weight Decay\n>\n> 降低模型复杂度防止过拟合\n\n`optimizer = optim.SGD( ..., weight_decay = 0.01)`\n\n## 动量\n\n> 动量-惯性\n>\n> momentum\n>\n> 在向后传播的过程中减去动量\n\n- ReduceLROnPlateau\n\n  > 训练过程中连续不下降，降低学习率\n\n## 动态学习率\n\n## DropOut\n\n> 忽略一部分 w 值\n>\n> `nn.Dropout(0.5)`\n"}}]);