(window.webpackJsonp=window.webpackJsonp||[]).push([[305],{933:function(e,n){e.exports="\x3c!--\ntitle: 35-Scrapy入门\nsort:\n--\x3e\n\n> 集成爬虫框架\n\n## HelloWorld\n\n> `scrapy runspider demo.py -o quotes.json`\n\n```python\nimport scrapy\n\n# 继承scarpy.Spider\nclass QuotesSpider(scrapy.Spider):\n    name = 'quotes'\n    # 爬取网址列表\n    start_urls = [\n        'http://quotes.toscrape.com/tag/humor/',\n        'http://quotes.toscrape.com/tag/books/'\n    ]\n    # 默认处理函数是parse\n    def parse(self, response):\n        for quote in response.css('div.quote'):\n            # 第一个yield输出数据\n            yield {\n                'author': quote.xpath('span/small/text()').get(),\n                'text': quote.css('span.text::text').get(),\n            }\n        # 下一page\n        next_page = response.css('li.next a::attr(\"href\")').get()\n        if next_page is not None:\n            yield response.follow(next_page, self.parse)\n```\n\n## 入门\n\n> 新建项目\n>\n> `scrapy startproject scrapyLearn`\n>\n> 运行\n>\n> `scrapy crawl test`\n\n```python\nimport scrapy\n\nclass WebSpider(scrapy.Spider):\n    # 唯一的名字\n    name = \"test\"\n    # 返回一个可迭代的请求\n    def start_requests(self):\n        urls = [\n            'http://quotes.toscrape.com/page/1/',\n            'http://quotes.toscrape.com/page/2/',\n        ]\n        for url in urls:\n            # 指定回调函数\n            yield scrapy.Request(url=url, callback=self.parse)\n\t# 解析响应\n    def parse(self, response):\n        page = response.url.split(\"/\")[-2]\n        filename = f'quotes-{page}.html'\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n        self.log(f'Saved file {filename}')\n```\n\n### 更精简的写法\n\n```python\nimport scrapy\n\nclass WebSpider(scrapy.Spider):\n    name = \"test\"\n    start_urls = [\n        'http://quotes.toscrape.com/page/1/',\n        'http://quotes.toscrape.com/page/2/',\n    ]\n    def parse(self, response):\n        page = response.url.split(\"/\")[-2]\n        filename = f'quotes-{page}.html'\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n```\n\n## 提取数据\n\n`scrapy shell 'url'`\n"}}]);