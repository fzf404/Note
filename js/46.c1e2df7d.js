(window.webpackJsonp=window.webpackJsonp||[]).push([[46],{674:function(n,e){n.exports='\x3c!--\ntitle: 21-ResNet网络\nsort:\n--\x3e\n\n> 其内部的残差块使用了跳跃连接，缓解了在深度神经网络中增加深度带来的梯度消失问题。\n\n![image-20210518220802949](https://img-1257284600.cos.ap-beijing.myqcloud.com/2021/image-20210518220802949.png)\n\n## 模型类\n\n```python\nimport torch\nimport torch.nn as nn\n\n# 残差模块\nclass ResBlock(nn.Module):\n  def __init__(self, in_channel, out_channel):\n    super(ResBlock, self).__init__()\n    self.conv1 = nn.Conv2d(in_channel, out_channel, 3, 1, 1, bias=False)\n    self.bn1 = nn.BatchNorm2d(out_channel)\n    self.relu1 = nn.ReLU()\n    self.conv2 = nn.Conv2d(out_channel, out_channel, 3, 1, 1, bias=False)\n    self.bn2 = nn.BatchNorm2d(out_channel)\n    self.relu2 = nn.ReLU()\n\n  def forward(self, x):\n    res = x\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu1(x)\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = x + res     # 残差连接\n    x = self.relu2(x)\n    return x\n\nclass ResBlockDown(nn.Module):\n  def __init__(self, in_channel, out_channel):\n    super(ResBlockDown, self).__init__()\n    self.conv1 = nn.Conv2d(in_channel, out_channel, 3, 2, 1, bias=False)    # 步长为2\n    self.bn1 = nn.BatchNorm2d(out_channel)\n    self.relu1 = nn.ReLU()\n    self.conv2 = nn.Conv2d(out_channel, out_channel, 3, 1, 1, bias=False)\n    self.bn2 = nn.BatchNorm2d(out_channel)\n    self.relu2 = nn.ReLU()\n    self.pool = nn.Conv2d(in_channel, out_channel, 1, 2, 0, bias=False)\n  def forward(self, x):\n    res = x\n    res = self.pool(res)    # 对输入进行下采样\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu1(x)\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = x + res\n    x = self.relu2(x)\n    return x\n\nclass ResNet34(nn.Module):\n  def __init__(self):\n    super(ResNet34, self).__init__()\n    self.conv1 = nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n    self.bn1 = nn.BatchNorm2d(64)\n    self.relu1 = nn.ReLU()\n    self.pool1 = nn.MaxPool2d(3, 2, 1)\n    self.conv2 = nn.Sequential(\n      ResBlockDown(64, 64),\n      ResBlock(64, 64),\n      ResBlock(64, 64),\n    )\n    self.conv3 = nn.Sequential(\n      ResBlockDown(64, 128),\n      ResBlock(128, 128),\n      ResBlock(128, 128),\n      ResBlock(128, 128),\n    )\n    self.conv4 = nn.Sequential(\n      ResBlockDown(128, 256),\n      ResBlock(256, 256),\n      ResBlock(256, 256),\n      ResBlock(256, 256),\n      ResBlock(256, 256),\n      ResBlock(256, 256),\n    )\n    self.conv5 = nn.Sequential(\n      ResBlockDown(256, 512),\n      ResBlock(512, 512),\n      ResBlock(512, 512),\n    )\n    self.gap = nn.AdaptiveAvgPool2d(1)\n    self.fc = nn.Linear(512, 2)\n  def forward(self, x):\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu1(x)\n    x = self.pool1(x)\n    x = self.conv2(x)\n    x = self.conv3(x)\n    x = self.conv4(x)\n    x = self.conv5(x)\n    x = self.gap(x)\n    x = x.view(x.size(0), -1)\n    x = self.fc(x)\n    return x\n  def weight_init(self):\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        nn.init.xavier_normal_(m.weight)\n      elif isinstance(m, nn.BatchNorm2d):\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n\n# 训练\n# 训练\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nmodel = ResNet34()\nmodel.weight_init()\ndata_transform = transforms.Compose([\n  transforms.Resize([224, 224]),\n  transforms.ToTensor(),\n])\n\nEpoch = 10\nbatch_size = 32\nlr = 0.001\nloss_func = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\nscheduler = optim.lr_scheduler.StepLR(optimizer, 2, 0.5)\ntrain_data = data_set("./train", data_transform, train=True)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\nval_data = data_set("./val", data_transform, train=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\ndevice = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")\nmodel.to(device)\n\ndef fit(model, loader, train=True):\n  if train:\n    torch.set_grad_enabled(True)\n    model.train()\n  else:\n    torch.set_grad_enabled(False)\n    model.eval()\n  running_loss = 0.0\n  acc = 0.0\n  max_step = 0\n  for img, label in tqdm(loader, leave=False):\n    max_step += 1\n    if train:\n      optimizer.zero_grad()\n    label_pred = model(img.to(device, torch.float))\n    pred = label_pred.argmax(dim=1)\n    acc += (pred.data.cpu() == label.data).sum()\n    loss = loss_func(label_pred, label.to(device, torch.long))\n    running_loss += loss\n    if train:\n      loss.backward()\n      optimizer.step()\n  running_loss = running_loss / (max_step)\n  avg_acc = acc / ((max_step) * batch_size)\n  if train:\n    scheduler.step()\n  return running_loss, avg_acc\n```\n'}}]);