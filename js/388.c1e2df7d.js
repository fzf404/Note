(window.webpackJsonp=window.webpackJsonp||[]).push([[388],{1016:function(n,e){n.exports="\x3c!--\ntitle: Hadoop\nsort:\n--\x3e\n\n> [官方文档](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)\n\n## 概念\n\n- DFS: 分布式文件系统\n- MR: MapReduce: 数据分发计算汇总\n\n## 安装\n\n```bash\n# 依赖库\napt install pdsh openjdk-8-jdk-headless -y\n\n# 免密登录\nssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nchmod 0600 ~/.ssh/authorized_keys\n\n# 下载hadoop\nwget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz\ntar -zxvf hadoop-3.2.2.tar.gz\n\n# 配置环境变量\nvim /etc/profile\nexport PDSH_RCMD_TYPE=ssh\nexport HADOOP_HOME=/opt/hadoop-3.2.2\nexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\nsource /etc/profile\n\n# 配置hadoop\n\n# 头部插入\nvim etc/hadoop/hadoop-env.sh\nJAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n\n# configuration中插入\nvim etc/hadoop/core-site.xml\n<configuration>\n    <property>\n        <name>fs.defaultFS</name>\n        <value>hdfs://localhost:9000</value>\n    </property>\n</configuration>\n\n# configuration中插入\nvim etc/hadoop/hdfs-site.xml\n<configuration>\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n    </property>\n</configuration>\n\n# 头部插入\nvim sbin/start-dfs.sh\n\nHDFS_NAMENODE_USER=root\nHDFS_DATANODE_USER=root\nHDFS_SECONDARYNAMENODE_USER=root\n\n# 运行前需指定为bash\nchsh -s /usr/bin/bash\n\n# 修改脚本 - 增加!\nvim libexec/hadoop-functions.sh\nif [[ ! -e '/usr/bin/pdsh' ]]; then\n\n# 格式化文件系统\nhadoop namenode -format\n\n# 启动\nsbin/start-dfs.sh\n# 停止\nsbin/stop-dfs.sh\n```\n\n## 使用\n\n```bash\n# 新建文件夹\nbin/hadoop fs -mkdir -R /user\n# 显示信息\nbin/hadoop fs -ls\n\n# 官方文档\nbin/hdfs dfs -mkdir input\nbin/hdfs dfs -put etc/hadoop/*.xml input\n\n# 官方示例\nbin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output 'dfs[a-z.]+'\n# 查看输出内容\nbin/hadoop fs -cat output/*\n```\n\n## 示例\n\n> [官方文档](https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)\n\n### 编写\n\n> 《三体》分词\n\n```java\npackage com.fzf404;\n\nimport java.io.IOException;\nimport java.util.List;\n\nimport com.github.houbb.segment.api.ISegmentResult;\nimport com.github.houbb.segment.bs.SegmentBs;\nimport com.github.houbb.segment.support.segment.result.impl.SegmentResultHandlers;\nimport com.github.houbb.segment.support.tagging.pos.tag.impl.SegmentPosTaggings;\nimport com.github.houbb.segment.util.SegmentHelper;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class WordCount {\n\n    public static class TokenizerMapper\n            extends Mapper<Object, Text, Text, IntWritable>{\n\n        private final static IntWritable one = new IntWritable(1);\n        private final Text word = new Text();\n\n        public void map(Object key, Text value, Context context\n        ) throws IOException, InterruptedException {\n            // 分词\n            List<String> resultList = SegmentHelper.segment(value.toString(), SegmentResultHandlers.word());\n            // 遍历\n            for(String itr:resultList){\n                // 词性\n                List<ISegmentResult> itrSegment= SegmentBs.newInstance().posTagging(SegmentPosTaggings.simple()).segment(itr);\n                // 切割词性\n                int cutIndex=itrSegment.toString().indexOf(\"/\");\n                // 判断名词\n                if(itrSegment.toString().substring(cutIndex + 1).equals(\"n]\")){\n                    context.write(new Text(itr), one);\n                }\n            }\n            // 英文分词\n//            StringTokenizer itr = new StringTokenizer(value.toString());\n//            while (itr.hasMoreTokens()) {\n//                word.set(itr.nextToken());\n//                context.write(word, one);\n//            }\n        }\n    }\n\n    public static class IntSumReducer\n            extends Reducer<Text,IntWritable,Text,IntWritable> {\n        private final IntWritable result = new IntWritable();\n\n        public void reduce(Text key, Iterable<IntWritable> values,\n                           Context context\n        ) throws IOException, InterruptedException {\n            int sum = 0;\n            for (IntWritable val : values) {\n                sum += val.get();\n            }\n            result.set(sum);\n            context.write(key, result);\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf, \"word count\");\n        job.setJarByClass(WordCount.class);\n        job.setMapperClass(TokenizerMapper.class);\n        job.setCombinerClass(IntSumReducer.class);\n        job.setReducerClass(IntSumReducer.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(IntWritable.class);\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n```\n\n### maven\n\n```xml\n<dependencies>\n    \x3c!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --\x3e\n    <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-mapreduce-client-core</artifactId>\n        <version>3.2.2</version>\n    </dependency>\n    \x3c!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --\x3e\n    <dependency>\n        <groupId>org.apache.hadoop</groupId>\n        <artifactId>hadoop-common</artifactId>\n        <version>3.2.2</version>\n    </dependency>\n    <dependency>\n        <groupId>com.github.houbb</groupId>\n        <artifactId>segment</artifactId>\n        <version>0.1.8</version>\n    </dependency>\n</dependencies>\n```\n\n### 打包\n\n```\nPorject Structure -> Artifacts -> Add -> OK\nBuild -> Artifacts -> jar -> Build\n```\n\n### 运行\n\n```bash\n# 拷贝数据\nbin/hadoop fs -put threebody.txt input\n# 清空output\nbin/hadoop fs -rm -r /user/root/output\n# 运行\nbin/hadoop jar wordcount.jar com.fzf404.WordCount /user/root/input /user/root/output\n# 查看结果\nbin/hadoop fs -cat /user/root/output/*\n```\n"}}]);