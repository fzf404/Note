(window.webpackJsonp=window.webpackJsonp||[]).push([[40],{668:function(n,t){n.exports="\x3c!--\ntitle: 12-Pytorch实战\nsort:\n--\x3e\n\n## MNIST\n\n```python\nimport torch\nimport numpy as np\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport torchvision\n\nw1, b1 = torch.randn(200, 784, requires_grad=True),\\\n    torch.zeros(200, requires_grad=True)\nw2, b2 = torch.randn(200, 200, requires_grad=True),\\\n    torch.zeros(200, requires_grad=True)\nw3, b3 = torch.randn(10, 200, requires_grad=True),\\\n    torch.zeros(10, requires_grad=True)\n\n# 数据初始化 kaiming-对于relu友好\ntorch.nn.init.kaiming_normal_(w1)\ntorch.nn.init.kaiming_normal_(w2)\ntorch.nn.init.kaiming_normal_(w3)\n\n# 向前传播\ndef forward(x):\n    x = x@w1.t()+b1\n    x = F.relu(x)\n    x = x@w2.t()+b2\n    x = F.relu(x)\n    x = x@w3.t()+b3\n    x = F.relu(x)\n    return x\n\nlearning_rate = 0.01\nepochs = 10\nbatch_size = 64\n\n# 数据初始化\ntrain_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('datasets/mnist_data',\n                train=True,\n                download=True,\n                transform=torchvision.transforms.Compose([\n                torchvision.transforms.ToTensor(),\n                torchvision.transforms.Normalize((0.1307, ), (0.3081, ))\n    ])), batch_size=batch_size,shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST('datasets/mnist_data/',\n                train=False,\n                download=True,\n                transform=torchvision.transforms.Compose([\n                torchvision.transforms.ToTensor(),\n                torchvision.transforms.Normalize((0.1307, ), (0.3081, ))\n    ])),batch_size=batch_size,shuffle=False)\n\n# 优化器\n# 实现随机梯度下降算法\noptimizer = optim.SGD([w1, b1, w2, b2, w3, b3], lr=learning_rate)\n# 交叉熵-损失函数\ncriteon = nn.CrossEntropyLoss()\n# 开始训练\nfor epoch in range(epochs):\n  # batch的索引, 图像数据， 标签\n  for batch_idx, (data, target) in enumerate(train_loader):\n      # 变换形状\n      data = data.view(-1, 28*28)\n      # 向前传播\n      logits = forward(data)\n      # 计算梯度\n      loss = criteon(logits, target)\n      # 清空梯度\n      optimizer.zero_grad()\n      # 先后传播\n      loss.backward()\n      optimizer.step()\n\n      if batch_idx % 100 == 0:\n        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n            epoch, batch_idx * len(data), len(train_loader.dataset),\n              100. * batch_idx / len(train_loader), loss.item()))\n\n  test_loss = 0\n  correct = 0\n  for data, target in test_loader:\n    data = data.view(-1, 28 * 28)\n    logits = forward(data)\n    test_loss += criteon(logits, target).item()\n\n    pred = logits.data.max(1)[1]\n    correct += pred.eq(target.data).sum()\n\n  test_loss /= len(test_loader.dataset)\n  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n      test_loss, correct, len(test_loader.dataset),\n      100. * correct / len(test_loader.dataset)))\n\n# fzf奇怪の验证代码\nfor data, target in test_loader:\n  data = data.view(-1, 28 * 28)[1]\n  pred = forward(data).argmax()\n  print(pred)\n  plt.imshow(data.view(28,28))\n  break\n```\n\n## 高级 API\n\n```python\nclass MLP(nn.Module):\n\n    def __init__(self):\n        super(MLP, self).__init__()\n\t\t\t\t# 定义连接层\n        self.model = nn.Sequential(\n            nn.Linear(784, 200),\n            nn.LeakyReLU(inplace=True),\n            nn.Linear(200, 200),\n            nn.LeakyReLU(inplace=True),\n            nn.Linear(200, 10),\n            nn.LeakyReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n# GPU加速\ndevice = torch.device('cuda:0')\nnet = MLP().to(device)\n# 重新定义优化器\noptimizer = optim.SGD(net.parameters(), lr=learning_rate)\n\n+ data, target = data.to(device), target.to(device)\n- logits = forward(data)\n+ logits = net(data)\n```\n\n## 调整\n\n```python\n# 正则化\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay = 0.01)\n\n# 动量\n# 连续200次loss没有减少，就将学习率×70%\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nscheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=200, factor=0.7)\n\tscheduler.step(loss)\n# 每执行800次，就将学习率×90%\nscheduler = StepLR(optimizer, step_size=800, gamma=0.9)\n\tscheduler.step()\n\n# DropOut\nself.model = nn.Sequential(\n  nn.Linear(784, 200),\n  nn.Dropout(0.5),  # 以0.5的概率断开\n  nn.LeakyReLU(inplace=True),\n  nn.Linear(200, 200),\n  nn.Dropout(0.5),  # 以0.5的概率断开\n  nn.LeakyReLU(inplace=True),\n  nn.Linear(200, 10),\n  nn.LeakyReLU(inplace=True),\n)\n# 测试集取消DropOut\n\tnet.eval()\n```\n"}}]);